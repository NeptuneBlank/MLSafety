{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Anomaly Detection"
      ],
      "metadata": {
        "id": "RcZg-ysGbLym"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Note:\n",
        "It is recommended to run the code using a GPU. To do this, go to Runtime > Change runtime type > Hardware Accelerator > select GPU.\n",
        "\n",
        "It may be the case that a GPU is not available, in which case, use a default CPU (\"None\" hardware accelerator). The code will still work without a GPU, but may run much slower.\n",
        "\n",
        "### Contributers:\n",
        "Jason Ding"
      ],
      "metadata": {
        "id": "rVDP9Kt-EoJX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pre-requisites\n",
        "\n",
        "In the following cells, we are installing and importing the necessary libaries and downloading the classification model."
      ],
      "metadata": {
        "id": "xTF95S98RyLT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "qo_2N4BQ3vdQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a945d272-a7fb-4453-b6e2-a40807ab64d3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-05-08 00:49:00--  https://github.com/hendrycks/outlier-exposure/raw/master/CIFAR/snapshots/baseline/cifar10_wrn_baseline_epoch_99.pt\n",
            "Resolving github.com (github.com)... 140.82.121.3\n",
            "Connecting to github.com (github.com)|140.82.121.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/hendrycks/outlier-exposure/master/CIFAR/snapshots/baseline/cifar10_wrn_baseline_epoch_99.pt [following]\n",
            "--2023-05-08 00:49:00--  https://raw.githubusercontent.com/hendrycks/outlier-exposure/master/CIFAR/snapshots/baseline/cifar10_wrn_baseline_epoch_99.pt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.111.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 9037421 (8.6M) [application/octet-stream]\n",
            "Saving to: ‘cifar10_wrn_baseline_epoch_99.pt’\n",
            "\n",
            "cifar10_wrn_baselin 100%[===================>]   8.62M  --.-KB/s    in 0.04s   \n",
            "\n",
            "2023-05-08 00:49:01 (204 MB/s) - ‘cifar10_wrn_baseline_epoch_99.pt’ saved [9037421/9037421]\n",
            "\n",
            "--2023-05-08 00:49:01--  https://raw.githubusercontent.com/hendrycks/pre-training/master/robustness/adversarial/models/wrn_with_pen.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.108.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3913 (3.8K) [text/plain]\n",
            "Saving to: ‘wrn_with_pen.py’\n",
            "\n",
            "wrn_with_pen.py     100%[===================>]   3.82K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-05-08 00:49:02 (62.8 MB/s) - ‘wrn_with_pen.py’ saved [3913/3913]\n",
            "\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.0.0+cu118)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.15.1+cu118)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.11.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.5.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.0.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.12.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.1)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (16.0.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.22.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision) (2.27.1)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (8.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (3.4)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2.0.12)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2022.12.7)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"  # It says this is to identify the specific API call causing the error\n",
        "\n",
        "\n",
        "!wget https://github.com/hendrycks/outlier-exposure/raw/master/CIFAR/snapshots/baseline/cifar10_wrn_baseline_epoch_99.pt\n",
        "!wget https://raw.githubusercontent.com/hendrycks/pre-training/master/robustness/adversarial/models/wrn_with_pen.py\n",
        "# !pip3 install torchvision==0.12.0 # Because this version of Pytorch does not work well w/ current version of CUDA \n",
        "\n",
        "# Install the latest version of pytorch (without specifying the CUDA version )\n",
        "!pip install torch torchvision\n",
        "\n",
        "# !pip install torch torchvision -f https://download.pytorch.org/whl/cu102/torch_stable.html\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Uninstall the current version of Pytorch \n",
        "# !pip uninstall torch torchvision -y\n"
      ],
      "metadata": {
        "id": "YOrfR6NTZfIB"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Install the latest version of pytorch (without specifying the CUDA version )\n",
        "# !pip install torch torchvision\n",
        "\n",
        "# !pip install torch torchvision -f https://download.pytorch.org/whl/cu102/torch_stable.html\n"
      ],
      "metadata": {
        "id": "dd0JDatLYYE6"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "pRRReMOZyRLW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cb192ae1-19d6-48d2-9563-152c72f81802"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch version: 2.0.0+cu118\n",
            "CUDA version: 11.8\n",
            "cuDNN version: 8700\n",
            "PyTorch Version:  2.0.0+cu118\n",
            "CUDA Version:  11.8\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "print(\"PyTorch version:\", torch.__version__)\n",
        "print(\"CUDA version:\", torch.version.cuda)\n",
        "print(\"cuDNN version:\", torch.backends.cudnn.version())\n",
        "import math\n",
        "from torch.utils.data import Dataset\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.transforms import ToTensor\n",
        "from torchvision import datasets\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "import sklearn.metrics as sk   # TO calculate AUROC score w/ sklearn \n",
        "from wrn_with_pen import WideResNet\n",
        "prefetch = 2\n",
        "print(\"PyTorch Version: \", torch.__version__)\n",
        "print(\"CUDA Version: \", torch.version.cuda)  # We want to know whether the GPU we are using is compatible to the current Pytorch version "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# check the GPU on your Colab instance\n",
        "!nvidia-smi  \n",
        "# and here to check CUDA compatibility for specific GPU model "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "68aoMfVKU25b",
        "outputId": "ed188a50-2a1f-4769-b7b4-adaff0269bd2"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mon May  8 00:49:11 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  NVIDIA A100-SXM...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   29C    P0    44W / 400W |      0MiB / 40960MiB |      0%      Default |\n",
            "|                               |                      |             Disabled |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model and Data Loading\n",
        "\n",
        "In the following cells, we are loading the CIFAR-10 model. Additionally, we are retrieving CIFAR-10 and out-of-distribution datasets."
      ],
      "metadata": {
        "id": "qu6Z4vj6RJJl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "M4dZ_8Es3cFw"
      },
      "outputs": [],
      "source": [
        "# Load CIFAR-10 model\n",
        "\n",
        "net = WideResNet(depth=40, num_classes=10, widen_factor=2, dropRate=0.3)\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    net.load_state_dict(torch.load('cifar10_wrn_baseline_epoch_99.pt'))\n",
        "    net.eval()\n",
        "    net.cuda()\n",
        "else:\n",
        "    net.load_state_dict(torch.load('cifar10_wrn_baseline_epoch_99.pt', map_location=torch.device('cpu')))\n",
        "    net.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "6n7lrVCvCDN3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "23a76698-1264-4b43-921f-aa53b74fc268"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170498071/170498071 [00:05<00:00, 29582287.59it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/cifar-10-python.tar.gz to data\n",
            "Downloading http://ufldl.stanford.edu/housenumbers/test_32x32.mat to data/test_32x32.mat\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 64275384/64275384 [00:06<00:00, 9458314.87it/s] \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz to data/cifar-100-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 169001437/169001437 [00:05<00:00, 29636373.48it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/cifar-100-python.tar.gz to data\n"
          ]
        }
      ],
      "source": [
        "# /////////////// Loading Datasets ///////////////\n",
        "mean = [x / 255 for x in [125.3, 123.0, 113.9]]\n",
        "std = [x / 255 for x in [63.0, 62.1, 66.7]]\n",
        "\n",
        "# /////////////// CIFAR-10 ///////////////\n",
        "\n",
        "data_transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize(mean, std)])\n",
        "\n",
        "cifar_10_data = datasets.CIFAR10(\n",
        "    root=\"data\",\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=data_transform\n",
        ")\n",
        "\n",
        "cifar10_data = cifar_10_data\n",
        "cifar10_loader = torch.utils.data.DataLoader(cifar10_data, batch_size=200, shuffle=False,\n",
        "                                          num_workers=prefetch, pin_memory=True)\n",
        "ood_num_examples = len(cifar10_data) // 5\n",
        "\n",
        "# /////////////// Rademacher Noise ///////////////\n",
        "\n",
        "dummy_targets = torch.ones(ood_num_examples)\n",
        "ood_data = torch.from_numpy(np.random.binomial(\n",
        "    n=1, p=0.5, size=(ood_num_examples, 3, 32, 32)).astype(np.float32)) * 2 - 1\n",
        "rademacher_ood_data = torch.utils.data.TensorDataset(ood_data, dummy_targets)\n",
        "rademacher_ood_loader = torch.utils.data.DataLoader(rademacher_ood_data, batch_size=200, shuffle=True)\n",
        "\n",
        "# /////////////// SVHN ///////////////\n",
        "\n",
        "data_transform = transforms.Compose([transforms.Resize(32), transforms.ToTensor(), transforms.Normalize(mean, std)])\n",
        "\n",
        "svhn_ood_data = torchvision.datasets.SVHN(root = \"data\", \n",
        "                          split=\"test\",\n",
        "                          transform = data_transform, \n",
        "                          download = True)\n",
        "\n",
        "svhn_ood_loader = torch.utils.data.DataLoader(svhn_ood_data, batch_size=200, shuffle=True,\n",
        "                                         num_workers=prefetch, pin_memory=True)\n",
        "\n",
        "# /////////////// DTD ///////////////\n",
        "\n",
        "# data_transform = transforms.Compose([transforms.Resize(32), transforms.CenterCrop(32),\n",
        "#                                      transforms.ToTensor(), transforms.Normalize(mean, std)])\n",
        "\n",
        "# dtd_ood_data = torchvision.datasets.DTD(root = \"data\", \n",
        "#                           split=\"test\",\n",
        "#                           transform = data_transform, \n",
        "#                           download = True)\n",
        "\n",
        "# dtd_ood_loader = torch.utils.data.DataLoader(dtd_ood_data, batch_size=200, shuffle=True,\n",
        "#                                          num_workers=prefetch, pin_memory=True)\n",
        "\n",
        "# /////////////// CIFAR-100 ///////////////\n",
        "\n",
        "data_transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize(mean, std)])\n",
        "\n",
        "cifar100_ood_data = datasets.CIFAR100(\n",
        "    root=\"data\",\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=data_transform\n",
        ")\n",
        "\n",
        "cifar100_ood_loader = torch.utils.data.DataLoader(cifar100_ood_data, batch_size=200, shuffle=True,\n",
        "                                         num_workers=prefetch, pin_memory=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## General Functions\n",
        "\n",
        "The following cells define functions that we will use in the rest of the code. You do not need to do anything here."
      ],
      "metadata": {
        "id": "OmMQ8yfaSLWm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "bIeyVeYM6yVL"
      },
      "outputs": [],
      "source": [
        "concat = lambda x: np.concatenate(x, axis=0)\n",
        "to_np = lambda x: x.data.cpu().numpy() # Converting tensor on GPU (to firstly CPT) to a numpy array \n",
        "\n",
        "'''\n",
        "Calculates the anomaly scores for a portion of the given dataset. \n",
        "If a GPU is not available, will run on a smaller fraction of the\n",
        "dataset, so that calculations will be faster.\n",
        "\n",
        "loader: A DataLoader that contains the loaded data of a dataset\n",
        "anomaly_score_calculator: A function that takes in the output \n",
        "                          logit of a batch of data and/or the \n",
        "                          penultimate.\n",
        "model_net: The classifier model.\n",
        "usePenultimate: True if anomaly_score_calculator needs the \n",
        "                penultimate as a parameter. False otherwise.\n",
        "'''\n",
        "def get_ood_scores(loader, anomaly_score_calculator, model_net, use_penultimate = False):\n",
        "    _score = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (data, target) in enumerate(loader):\n",
        "            if torch.cuda.is_available():\n",
        "                fraction = 200\n",
        "            else:\n",
        "                fraction = 1000\n",
        "            if batch_idx >= ood_num_examples // fraction:\n",
        "                break\n",
        "\n",
        "            if torch.cuda.is_available():\n",
        "                data = data.cuda()\n",
        "\n",
        "            output = model_net(data)\n",
        "\n",
        "            if use_penultimate:\n",
        "                score = anomaly_score_calculator(output[0], output[1])\n",
        "            else:\n",
        "                score = anomaly_score_calculator(output[0])\n",
        "            _score.append(score)\n",
        "            # print(f\"Appended score shape: {score.shape}\")  # This was to ensure that \n",
        "    return concat(_score).copy()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# /////////////// Printing Results ///////////////\n",
        "all_anomaly_results = {}\n",
        "\n",
        "'''\n",
        "Returns and prints out the AUROC score of a dataset.\n",
        "\n",
        "ood_loader: A DataLoader that contains the loaded data of a dataset\n",
        "anomaly_score_calculator: A function that takes in the output \n",
        "                          logit of a batch of data and/or the \n",
        "                          penultimate.\n",
        "model_net: The classifier model.\n",
        "usePenultimate: True if anomaly_score_calculator needs the \n",
        "                penultimate as a parameter. False otherwise.\n",
        "'''\n",
        "def get_and_print_results(ood_loader, anomaly_score_calculator, model_net, use_penultimate):\n",
        "    out_score = get_ood_scores(ood_loader, anomaly_score_calculator, model_net, use_penultimate = use_penultimate)\n",
        "    auroc = get_auroc(out_score, in_score)\n",
        "    print('AUROC: \\t\\t\\t{:.2f}'.format(100 * auroc) + \"%\")\n",
        "    return auroc\n",
        "\n",
        "'''\n",
        "Prints out the AUROC score of all the OOD datasets. The results \n",
        "will be appended to global variable all_anomaly_results, which \n",
        "is used later for display purposes.\n",
        "\n",
        "anomaly_score_calculator: A function that takes in the output \n",
        "                          logit of a batch of data and/or the \n",
        "                          penultimate.\n",
        "anomaly_score_name: The name of the anomaly score method.\n",
        "model_net: The classifier model.\n",
        "model_name: The name of the classifier model.\n",
        "usePenultimate: True if anomaly_score_calculator needs the \n",
        "                penultimate as a parameter. False otherwise.\n",
        "'''\n",
        "def print_all_results(anomaly_score_calculator, anomaly_score_name, model_net, model_name = \"default_model\", use_penultimate = False):\n",
        "    global in_score, all_anomaly_results\n",
        "    in_score = get_ood_scores(cifar10_loader, anomaly_score_calculator, model_net, use_penultimate)\n",
        "    results = []\n",
        "\n",
        "    print('Rademacher Noise Detection')\n",
        "    auroc = get_and_print_results(rademacher_ood_loader, anomaly_score_calculator, model_net, use_penultimate)\n",
        "    results.append(auroc)\n",
        "\n",
        "    print('\\nSVHN Detection')\n",
        "    auroc = get_and_print_results(svhn_ood_loader, anomaly_score_calculator, model_net, use_penultimate)\n",
        "    results.append(auroc)\n",
        "\n",
        "    # print('\\nDTD Detection')\n",
        "    # auroc = get_and_print_results(dtd_ood_loader, anomaly_score_calculator, model_net, use_penultimate)\n",
        "    # results.append(auroc)\n",
        "\n",
        "    print('\\nCIFAR-100 Detection')\n",
        "    auroc = get_and_print_results(cifar100_ood_loader, anomaly_score_calculator, model_net, use_penultimate)\n",
        "    results.append(auroc)\n",
        "\n",
        "    average = sum(results) / len(results)\n",
        "    results.append(average)\n",
        "\n",
        "    if not model_name in all_anomaly_results:\n",
        "        all_anomaly_results[model_name] = {}\n",
        "    all_anomaly_results[model_name][anomaly_score_name] = results"
      ],
      "metadata": {
        "id": "uPaE51LaTJ88"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implement AUROC Score\n",
        "Fill in the get_auroc score. We will use this function in order to calculate the AUROC score of an out-of-distribution dataset.\n",
        "\n",
        "It may be helpful to use the sklearn.metrics.roc_auc_score() function. Both _pos and _neg should be used."
      ],
      "metadata": {
        "id": "epCT4PxHGJ3e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Calculates the AUROC score of a OOD dataset.\n",
        "\n",
        "_pos: an array of anomoly scores of the OOD dataset\n",
        "_neg: an array of anomoly scores of images in the CIFAR-10 dataset\n",
        "return: The AUROC score the data in decimal form\n",
        "'''\n",
        "def get_auroc(_pos, _neg):\n",
        "    ############################################################################\n",
        "    # TODO:  Calculate the AUROC score.                                        #\n",
        "    ############################################################################\n",
        "    # An ROC curve (receiver operating characteristic curve) is a graph showing the performance of a classification model at all classification thresholds. This curve plots two parameters: True Positive Rate. False Positive Rate.\n",
        "    # AUROC refers to the \"area underneath ROC curve\", and AUROC score is a decimal point that reflect tells us how efficient the model is. The higher the AUC, the better the model's performance at distinguishing between the positive and negative classes.\n",
        "    # AUROC score of an out-of-distribution dataset reflects how efficient the model is on out-of-distribution dataset\n",
        "    # sklearn.metrics.roc_auc_score takes in the true labels and predicted labels and then calculate an AUROC score \n",
        "\n",
        "    # auroc_score = sk.roc_auc_score(_pos, _neg)\n",
        "    # I might simply be wrong. I have no idea why should I put _pos and _neg here. \n",
        "\n",
        "    true_labels = np.concatenate([np.ones(len(_neg)), np.zeros(len(_pos))]) # Where 1 means labels to in-distrobution labels while 0 means labels to OOD labels \n",
        "    probability_estimates = np.concatenate([_neg, _pos]) # It seems both true labels and predicted labels are from the same data sources? Very interresting & somehow makes sense to me \n",
        "\n",
        "    auroc_score = sk.roc_auc_score(true_labels, probability_estimates) # Note that an error here once incurred repetitive reference of this function by itself when you wrote auroc_score = get_auroc(true_labels, probability_estimates)\n",
        "    ############################################################################\n",
        "    #                             END OF YOUR CODE                             #\n",
        "    ############################################################################\n",
        "    return auroc_score # Should be a numpy array, given all above is. "
      ],
      "metadata": {
        "id": "FFLwtHgRFWK5"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implement Anomaly Score Calculators\n",
        "Fill in the folllowing functions which calculate the anomaly score given the model's output for a batch of data (the output will contain one logit per image in the batch).\n",
        "\n",
        "The following equations show how the logits should be transformed in order to get the anomaly score.\n",
        "\n",
        "Max Logit Equation: <br>\n",
        "$\\text{Score}=-\\text{max} l_k$\n",
        "\n",
        "Max Softmax Equation: <br>\n",
        "$\\text{Score}=-\\text{max} p(y=k|x)$\n",
        "\n",
        "Cross Entropy Anomaly Equation: <br>\n",
        "$\\text{Score} = \\bar{l}-\\text{log}∑_{c=1}^{\\text{num_classes}}e^{l_c}$"
      ],
      "metadata": {
        "id": "Ot-KkzW1K55s"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "Lajsgp2BXnc2"
      },
      "outputs": [],
      "source": [
        "# /////////////// Anomaly Score Calculators ///////////////\n",
        "\n",
        "# 这里我的一些问题：\n",
        "# 我不能理解anomaly detection score calculator的目的和流程\n",
        "# 对于提供好的公式，我不清楚每一个notations都代指什么\n",
        "\n",
        "'''\n",
        "Calculates the max logit anomaly score of a batch of outputs.\n",
        "\n",
        "output: The model's output for a batch of data.\n",
        "'''\n",
        "def max_logit_anomaly_score(output):\n",
        "    ############################################################################\n",
        "    # TODO: Calculate the max logit anomaly score (returning a numpy array).   #\n",
        "    ############################################################################\n",
        "    # Max logit equation \n",
        "    # $l_k$ refers to the logit (log-odds) values by model for each class. \"odds\"= P(events happening)/P(events not happenning). Log-odds simply means taking a natural log. \n",
        "    # print(f'dim of output is {output.shape}')\n",
        "    score, index = torch.max(output, dim = 1) # Calculate the maximum out of a tensor along its second axis. The output will contain one logit per image in the batch. Score should be a tensor of (200,1) dimension\n",
        "    # torch.max produces both max and its index \n",
        "  \n",
        "    score = to_np (score) # Move the tensor to CPU before converting it to a numpy array\n",
        "\n",
        "    score = - score # As instructed \n",
        "     \n",
        "    ############################################################################\n",
        "    #                             END OF YOUR CODE                             #\n",
        "    ############################################################################\n",
        "    return score\n",
        "\n",
        "'''\n",
        "Calculates the max softmax anomaly score of a batch of outputs.\n",
        "\n",
        "output: The model's output for a batch of data.\n",
        "'''\n",
        "def max_softmax_anomaly_score(output):\n",
        "    ############################################################################\n",
        "    # TODO: Calculate the max softmax anomaly score (returning a numpy array). #\n",
        "    ############################################################################\n",
        "    # Max softmax equation\n",
        "    # First you'll need to convert logits into probabilities. To do so, you will need to calculate softmax. \n",
        "    probabilities = F.softmax(output, dim=1) # dim=1 because you want to calculate the softmax along the dim of class and that was the sec dimension. \n",
        "    score, index = torch.max(probabilities, dim=1)  # Calculate the maximum along the dim of class \n",
        "\n",
        "    score = to_np (score)\n",
        "    soure = - score \n",
        "    ############################################################################\n",
        "    #                             END OF YOUR CODE                             #\n",
        "    ############################################################################\n",
        "    return score\n",
        "\n",
        "'''\n",
        "Calculates the cross entropy anomaly score of a batch of outputs.\n",
        "\n",
        "output: The model's output for a batch of data.\n",
        "'''\n",
        "def cross_entropy_anomaly_score(output):\n",
        "    ############################################################################\n",
        "    # TODO: Calculate the cross entropy anomaly score (returning a numpy array).#\n",
        "    ############################################################################\n",
        "    # Calculate l_bar (average logit values)\n",
        "    l_bar = torch.mean(output)  #l_bar is the average of all logit values\n",
        "\n",
        "    # Calculate the log_sum \n",
        "    log_sum = torch.log(torch.sum(torch.exp(output), dim=1)) # Note that you want to sum up along the sec dim\n",
        "\n",
        "    # Calculate cross_entropy_anomaly_score \n",
        "    score = l_bar - log_sum\n",
        "\n",
        "    score = to_np(score)\n",
        "\n",
        "    ############################################################################\n",
        "    #                             END OF YOUR CODE                             #\n",
        "    ############################################################################\n",
        "    return score"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Print AUROC Results\n",
        "\n",
        "Run the following cells in order to see how well each of the anomaly score calculators do on the OOD datasets."
      ],
      "metadata": {
        "id": "tmqvyCn-X3RE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "Wz78Dx8PM05O",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "90b3aeda-b64e-4467-e121-5dbf5fc71744"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======= Max Logit AUROC Scores =======\n",
            "Rademacher Noise Detection\n",
            "AUROC: \t\t\t29.13%\n",
            "\n",
            "SVHN Detection\n",
            "AUROC: \t\t\t9.16%\n",
            "\n",
            "CIFAR-100 Detection\n",
            "AUROC: \t\t\t12.58%\n"
          ]
        }
      ],
      "source": [
        "print(\"======= Max Logit AUROC Scores =======\")\n",
        "print_all_results(max_logit_anomaly_score, \"Max Logit\", net)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "jKL87S6dMyFY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1d9e6486-ea13-4979-94dd-01dff8077870"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======= Max Softmax AUROC Scores =======\n",
            "Rademacher Noise Detection\n",
            "AUROC: \t\t\t80.56%\n",
            "\n",
            "SVHN Detection\n",
            "AUROC: \t\t\t91.62%\n",
            "\n",
            "CIFAR-100 Detection\n",
            "AUROC: \t\t\t87.98%\n"
          ]
        }
      ],
      "source": [
        "print(\"======= Max Softmax AUROC Scores =======\")\n",
        "print_all_results(max_softmax_anomaly_score, \"Max Softmax\", net)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "Yosj6ofYV9bs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1eb6ae34-fe89-4af9-c9ca-70263795e7e9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======= Cross Entropy AUROC Scores =======\n",
            "Rademacher Noise Detection\n",
            "AUROC: \t\t\t29.09%\n",
            "\n",
            "SVHN Detection\n",
            "AUROC: \t\t\t8.67%\n",
            "\n",
            "CIFAR-100 Detection\n",
            "AUROC: \t\t\t12.61%\n"
          ]
        }
      ],
      "source": [
        "print(\"======= Cross Entropy AUROC Scores =======\")\n",
        "print_all_results(cross_entropy_anomaly_score, \"Cross Entropy\", net)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implement ViM\n",
        "\n",
        "We will now implement virtual-logit matching (ViM). ViM works by doing the following. \n",
        "\n",
        "ViM first considers the **principal space** ($P$) of the features obtained by passing the training data (CIFAR-10) through to just before the final fully-connected layer. These features are then centered at the origin of a new coordinate system (defined by `u` in our code below). Let us use the 12 most significant principal components of these features for our principal space.\n",
        "\n",
        "ViM calculates **alpha** ($\\alpha$) by projecting the training data's features onto the space orthogonal to the prinicpal space ($P^\\perp$). This projection is known as the **residual** ($x^{P^\\perp}$). HINT: You may find it helpful to use np.linalg.svd to find the principal components (or you can use eignvalues and eigenvectors). The space orthogonal to the principle space can be found simply by taking the remaining principal components (i.e. all PCs apart from the first 12).\n",
        "\n",
        "Alpha is then calculated by the following equation:\n",
        "> $\\alpha := \\frac{∑^k_{i=1}\\text{max}_{j=1,...,C}\\{l^i_j\\}}{∑^K_{i=1}\\| x_i^{P\\perp} \\|}$\n",
        "\n",
        "In other words, alpha is the sum of each logit's maximum value, divided by the sum of the norms of each feature's residuals.\n",
        "\n",
        "Your calculated alpha in this part should be around 16.16.\n"
      ],
      "metadata": {
        "id": "nq3lnDCbZ6qK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "erJ538Vl1qYy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8650f65e-606c-4643-f01e-fe7366654ec9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Computing principal_space_perp...\n",
            "Computing alpha...\n",
            "alpha = 16.28367042541504\n"
          ]
        }
      ],
      "source": [
        "# You may find the following functions useful.\n",
        "from numpy.linalg import pinv, norm\n",
        "from scipy.special import logsumexp\n",
        "\n",
        "_score = []\n",
        "to_np = lambda x: x.data.cpu().numpy()\n",
        "concat = lambda x: np.concatenate(x, axis=0)\n",
        "\n",
        "# Extraction fully connected layer's weights and biases\n",
        "w, b = net.fc.weight.cpu().detach().numpy(), net.fc.bias.cpu().detach().numpy()\n",
        "# Origin of a new coordinate system of feature space to remove bias\n",
        "\n",
        "\n",
        "'''\n",
        "Calculates and returns the space orthogonal to the principal space (principal_space_perp)\n",
        "and alpha values given the training data the model used.\n",
        "\n",
        "\n",
        "training_data_loader: A DataLoader that contains the loaded data of a \n",
        "                      training dataset.\n",
        "model_net: The classifier model.\n",
        "verbose: If true, will print out the alpha value.\n",
        "return: Returns the alpha value.\n",
        "'''\n",
        "def compute_ViM_principal_space_perp_and_alpha(training_data_loader, model_net, verbose = False):\n",
        "    result = []\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # Getting the first batch of the training data to calculate principal_space_perp and alpha\n",
        "    # Principle space: the 12 most significant components of features \n",
        "    # Residuals: The projection/remaining/space orthogonal to the PS: all remaining components\n",
        "    # Steps (i) PS (ii) Residuals (iii) alpha \n",
        "    training_data, target = next(iter(training_data_loader))\n",
        "    if torch.cuda.is_available():\n",
        "        training_data = training_data.cuda()\n",
        "    # Step 1: Passing the training data (CIFAR-10) through to just before the final fully-connected layer\n",
        "    result = model_net(training_data)\n",
        "\n",
        "    logit = result[0] # Logits (values before softmax) # Interesting. This time the logits are not given as \"output\", as it was for Anomaly Scores\n",
        "    penultimate = result[1] # Features/Penultimate (values before fully connected layer) # I think this penultimate is what is takes to calculate PS. \n",
        "\n",
        "    logit_id_train = logit.cpu().detach().numpy().squeeze()  # to CPU --> get rid of the gradients --> to numpy --> get rid of redundant dimensions\n",
        "    feature_id_train = penultimate.cpu().detach().numpy().squeeze() \n",
        "\n",
        "\n",
        "    # PCA算法相关的步骤我好像还不太懂\n",
        "    ############################################################################\n",
        "    # TODO:  Calculate the space orthogonal to the pricipal space and then     #\n",
        "    # compute alpha.                                                           #\n",
        "    ############################################################################\n",
        "    if verbose:  # What is the \"verbose\" condition?\n",
        "        u = np.mean(feature_id_train, axis=0)  # Adjust the center for coordinate \n",
        "        # Step 2: Features are then centered at the origin of a new coordinate system (defined by u in our code below)\n",
        "        centered_features = feature_id_train - u # My intuition is +u because u itself contains \"-\"\n",
        "        # Step 3: Use the 12 most significant principal components of these features for our principal space.\n",
        "        # Hint Use np.linalg.svd to find the principal components (or you can use eignvalues and eigenvectors).\n",
        "        print('Computing principal_space_perp...')  # This is residuals. I think they unnecessarily introduced too many concepts that say the same things. \n",
        "        u, s, vh = np.linalg.svd(centered_features, full_matrices=True, compute_uv=True, hermitian=False)   # HINT: You may find it helpful to use np.linalg.svd to find the principal components (or you can use eignvalues and eigenvectors).\n",
        "        # Taking the first 12 components \n",
        "        principal_space = vh[:12] # Let us use the 12 most significant principal components of these features for our principal space.\n",
        "\n",
        "        # Project the centered features onto the principal space:\n",
        "        projected_features = centered_features @ principal_space.T\n",
        "\n",
        "        # Reconstruct the features from the projected features using the principal space:\n",
        "        reconstructed_features = projected_features @ principal_space\n",
        "\n",
        "\n",
        "        # Step 4: Residuals \n",
        "        principal_space_perp = centered_features - reconstructed_features # \" The space orthogonal to the principle space can be found simply by taking the remaining principal components (i.e. all PCs apart from the first 12).\"\n",
        "\n",
        "    if verbose:\n",
        "        print('Computing alpha...')\n",
        "        # Step 5: Calculating alpha \n",
        "        alpha = np.sum(np.amax(logit_id_train, axis=1)) / np.sum(np.linalg.norm(principal_space_perp,axis=1)) #\n",
        "\n",
        "\n",
        "    ############################################################################\n",
        "    #                             END OF YOUR CODE                             #\n",
        "    ############################################################################\n",
        "    if verbose:\n",
        "        print(f'alpha = {alpha}')\n",
        "\n",
        "    return principal_space_perp, alpha\n",
        "\n",
        "principal_space_perp, alpha = compute_ViM_principal_space_perp_and_alpha(cifar10_loader, net, verbose = True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implement ViM Anomaly Score Calculator\n",
        "\n",
        "Now, implement the ViM anomaly score calculator. \n",
        "\n",
        "First, we want to project our penultimate/feature values onto principal_space_perp which we found before, which is called the residual. We multiply the norm of this residual by alpha to get what we call the **virtual logit score**. \n",
        "\n",
        "Next, we get what some call the **energy score** by taking LogSumExp of the logits. \n",
        "\n",
        "Finally, the outputted **anomaly score** is calculated by subtracting the virtual logit by the energy score.\n",
        "\n",
        "$\\text{vlogit}= \\alpha \\| {x^{P^\\perp}} \\|$\\\n",
        "$\\text{energy} = \\text{ln}\\sum_{i=1}^C e^{l_i}$\\\n",
        "$\\text{anomaly_score} = \\text{vlogit} - \\text{energy}$"
      ],
      "metadata": {
        "id": "rmP5powXLnM6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "jzikAO1QYRSh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7a5fd7df-a702-4191-bf56-21926fc8d050"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======= ViM_anomaly_score_calculator =======\n",
            "Computing principal_space_perp...\n",
            "Computing alpha...\n",
            "alpha = 16.28367042541504\n",
            "Rademacher Noise Detection\n",
            "AUROC: \t\t\t30.95%\n",
            "\n",
            "SVHN Detection\n",
            "AUROC: \t\t\t21.30%\n",
            "\n",
            "CIFAR-100 Detection\n",
            "AUROC: \t\t\t23.93%\n"
          ]
        }
      ],
      "source": [
        "'''\n",
        "Calculates the ViM anomaly score of a batch of outputs.\n",
        "\n",
        "output: The model's output for a batch of data.\n",
        "penultimate: The model's penultimate (feature values) for a batch of data.\n",
        "'''\n",
        "def ViM_anomaly_score_calculator(output, penultimate):\n",
        "    logit_id_val = output.cpu().detach().numpy().squeeze()\n",
        "    feature_id_val = penultimate.cpu().detach().numpy().squeeze()\n",
        "\n",
        "    ############################################################################\n",
        "    # TODO:  Calculate the anomaly score.                                      #\n",
        "    ############################################################################\n",
        "    # virtual logir score  # It's kinda weird to me that they offered different variables for different functions. Sometimes I need to do something repetitively accordingly. Either side should be wrong. \n",
        "    residuals_norm = torch.norm(torch.tensor(principal_space_perp), dim=1)  # Cal the norms along the dim of feature space\n",
        "    vlogit = alpha * residuals_norm\n",
        "    # energy score \n",
        "    energy = np.log(np.sum(np.exp(logit_id_val)))   \n",
        "    # anomaly score  # I didn't get it why we are calculating anomaly score once again. \n",
        "    anomaly_score = vlogit - energy \n",
        "\n",
        "    ############################################################################\n",
        "    #                             END OF YOUR CODE                             #\n",
        "    ############################################################################\n",
        "    \n",
        "    return anomaly_score\n",
        "\n",
        "print(\"======= ViM_anomaly_score_calculator =======\")\n",
        "w, b = net.fc.weight.cpu().detach().numpy(), net.fc.bias.cpu().detach().numpy()\n",
        "u = -np.matmul(pinv(w), b)\n",
        "principal_space_perp, alpha = compute_ViM_principal_space_perp_and_alpha(cifar10_loader, net, verbose = True) # Making sure you have the correct ViM values before calculating the score \n",
        "print_all_results(ViM_anomaly_score_calculator, \"ViM\", net, use_penultimate = True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Compare Anomaly Score Results\n",
        "Run the following cell to see how the different anomaly score calculators compare to each other for the OOD datasets. You should see that ViM is superior to other anomaly scores in all of the datasets except for CIFAR-10."
      ],
      "metadata": {
        "id": "jeNAD556O5gN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "HIJ-zCyvJYp9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fa46b1ae-b5bd-4ad1-f1e5-b40d56e13a28"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "            default_model |  Rademacher  |     SVHN     |  CIFAR-100   |   Average   \n",
            "=====================================================================================\n",
            "                Max Logit |    29.13%    |    9.16%     |    12.58%    |    16.96%   \n",
            "              Max Softmax |   *80.56%    |   *91.62%    |   *87.98%    |   *86.72%   \n",
            "            Cross Entropy |    29.09%    |    8.67%     |    12.61%    |    16.79%   \n",
            "                      ViM |    30.95%    |    21.30%    |    23.93%    |    25.39%   \n",
            "\n",
            "\n",
            "* highlights the maximum AUROC Score for an OOD Dataset\n"
          ]
        }
      ],
      "source": [
        "# ///////////////// Compare Results /////////////////\n",
        "\n",
        "def get_results_max(model_name = \"normal\"):\n",
        "    all_anomaly_results[model_name][\"max\"] = [0,0,0,0,0]\n",
        "    for key in all_anomaly_results[model_name].keys():\n",
        "        if (key != \"max\"):\n",
        "            index = 0\n",
        "            for score in all_anomaly_results[model_name][key]:\n",
        "                all_anomaly_results[model_name][\"max\"][index] = \\\n",
        "                    max(score, all_anomaly_results[model_name][\"max\"][index])\n",
        "                index += 1\n",
        "\n",
        "\n",
        "def compare_all_results():\n",
        "    for model_name in all_anomaly_results:\n",
        "        to_be_printed = \" \" * (25 - len(model_name)) + model_name\n",
        "        dataset_names = [\"Rademacher\", \"SVHN\", \"CIFAR-100\", \"Average\"]\n",
        "        for name in dataset_names:\n",
        "            to_be_printed += \" | \" + \" \"*(6-math.ceil(len(name)/2)) + \\\n",
        "                                name + \" \"*(6-math.floor(len(name)/2))\n",
        "\n",
        "        print(to_be_printed)\n",
        "        print(\"=\" * (25 + len(dataset_names) * 15))\n",
        "\n",
        "        get_results_max(model_name = model_name)\n",
        "        for key in all_anomaly_results[model_name].keys():\n",
        "            if (key != \"max\"):\n",
        "                to_be_printed = \" \"*(25-len(key)) + key\n",
        "                index = 0\n",
        "                for result in all_anomaly_results[model_name][key]:\n",
        "                    if (all_anomaly_results[model_name][\"max\"][index] == result):\n",
        "                        result = \"*\" + '{:.2f}'.format(round(result * 100, 2)) + \"%\"\n",
        "                    else:\n",
        "                        result = '{:.2f}'.format(round(result * 100, 2)) + \"%\"\n",
        "                    to_be_printed += \" | \" + \" \"*(6-math.ceil(len(result)/2)) + \\\n",
        "                                        result + \" \"*(6-math.floor(len(result)/2))\n",
        "                    index += 1\n",
        "                print(to_be_printed)\n",
        "        print()\n",
        "\n",
        "    print(\"\\n* highlights the maximum AUROC Score for an OOD Dataset\")\n",
        "\n",
        "compare_all_results()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Augmentation\n",
        "\n",
        "Now we will see if models trained using data augmentation methods for robustness help in OOD detection. \n",
        "\n",
        "We will load a CIFAR-10 model that used PixMix data augmentation during training, and see how it fares compared to the default model that did not use data augmentation."
      ],
      "metadata": {
        "id": "lOftK8KC8fSJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown 1mjIfbb3mfXXvAZ1sBnjotFr5yYFmLi68 # Downloading PixMix model\n",
        "!gdown 1skZT6yplO-Sv4M8Ksgzx14cTY3H-HgOa # Downlaoding wideresnet class"
      ],
      "metadata": {
        "id": "ANy7vzim9PbC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3c1b2d73-b387-4e0a-e25d-6656c3b23289"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1mjIfbb3mfXXvAZ1sBnjotFr5yYFmLi68\n",
            "To: /content/checkpoint.pth.tar\n",
            "100% 71.8M/71.8M [00:00<00:00, 73.4MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1skZT6yplO-Sv4M8Ksgzx14cTY3H-HgOa\n",
            "To: /content/wideresnet_with_pen.py\n",
            "100% 4.03k/4.03k [00:00<00:00, 25.0MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from wideresnet_with_pen import WideResNet as WideResNet2"
      ],
      "metadata": {
        "id": "wx8QTsLj9QAy"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading the PixMix model\n",
        "\n",
        "pixmix_net = WideResNet2(depth=40, num_classes=10, widen_factor=4, drop_rate=0.3)\n",
        "pixmix_net = torch.nn.DataParallel(pixmix_net)\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    checkpoint = torch.load('checkpoint.pth.tar')\n",
        "    pixmix_net.load_state_dict(checkpoint['state_dict'])\n",
        "    net.eval()\n",
        "    net.cuda()\n",
        "else:\n",
        "    checkpoint = torch.load('checkpoint.pth.tar', map_location=torch.device('cpu'))\n",
        "    pixmix_net.load_state_dict(checkpoint['state_dict'])\n",
        "    net.eval()"
      ],
      "metadata": {
        "id": "TQiyBmip9eYr"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PixMix Model Testing\n",
        "\n",
        "Let us now test the PixMix model with the same anomaly score calculators we coded before."
      ],
      "metadata": {
        "id": "XRje54Ip-w3p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"======= Max Logit AUROC Scores =======\")\n",
        "print_all_results(max_logit_anomaly_score, \"Max Logit\", pixmix_net, model_name = \"pixmix_trained_model\")"
      ],
      "metadata": {
        "id": "W_MVibEl9mgS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "329e3e9e-ff40-4298-dd01-761837033333"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======= Max Logit AUROC Scores =======\n",
            "Rademacher Noise Detection\n",
            "AUROC: \t\t\t5.59%\n",
            "\n",
            "SVHN Detection\n",
            "AUROC: \t\t\t7.36%\n",
            "\n",
            "CIFAR-100 Detection\n",
            "AUROC: \t\t\t12.52%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"======= Max Softmax Probability AUROC Scores =======\")\n",
        "print_all_results(max_softmax_anomaly_score, \"Max Softmax Probability\", pixmix_net, model_name = \"pixmix_trained_model\")"
      ],
      "metadata": {
        "id": "q3Nx8ieU9npx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "22221c5d-a4ae-42e5-95ce-1c2369ac12d7"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======= Max Softmax Probability AUROC Scores =======\n",
            "Rademacher Noise Detection\n",
            "AUROC: \t\t\t92.81%\n",
            "\n",
            "SVHN Detection\n",
            "AUROC: \t\t\t91.36%\n",
            "\n",
            "CIFAR-100 Detection\n",
            "AUROC: \t\t\t87.57%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"======= Cross Entropy AUROC Scores =======\")\n",
        "print_all_results(cross_entropy_anomaly_score, \"Cross Entropy\", pixmix_net, model_name = \"pixmix_trained_model\")"
      ],
      "metadata": {
        "id": "umUyonHj9oGm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1a046a08-6ef5-4da1-df2a-5eb7097d715e"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======= Cross Entropy AUROC Scores =======\n",
            "Rademacher Noise Detection\n",
            "AUROC: \t\t\t5.77%\n",
            "\n",
            "SVHN Detection\n",
            "AUROC: \t\t\t7.88%\n",
            "\n",
            "CIFAR-100 Detection\n",
            "AUROC: \t\t\t13.62%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"======= ViM_anomaly_score_calculator =======\")\n",
        "w, b = pixmix_net.module.fc.weight.cpu().detach().numpy(), pixmix_net.module.fc.bias.cpu().detach().numpy()\n",
        "u = -np.matmul(pinv(w), b)\n",
        "principal_space_perp, alpha = compute_ViM_principal_space_perp_and_alpha(cifar10_loader, pixmix_net, verbose = True)\n",
        "print_all_results(ViM_anomaly_score_calculator, \"ViM\", pixmix_net, model_name = \"pixmix_trained_model\", use_penultimate = True)"
      ],
      "metadata": {
        "id": "mPyLCKXK9rtx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aa982277-56de-4c94-84b5-16f62aea1729"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======= ViM_anomaly_score_calculator =======\n",
            "Computing principal_space_perp...\n",
            "Computing alpha...\n",
            "alpha = 12.416947364807129\n",
            "Rademacher Noise Detection\n",
            "AUROC: \t\t\t5.31%\n",
            "\n",
            "SVHN Detection\n",
            "AUROC: \t\t\t8.22%\n",
            "\n",
            "CIFAR-100 Detection\n",
            "AUROC: \t\t\t18.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Compare No-data-augmentation Model vs PixMix Model\n",
        "\n",
        "Let us now compare how the default model compared to the PixMix model by running the following cell. \n",
        "\n",
        "You should see that the PixMix model successfully helps us in OOD detection, and has a higher AUROC score (except for ViM). This suggests that one should evaluate models across numerous OOD datasets to soundly assess OOD detection performance."
      ],
      "metadata": {
        "id": "jt-XKgcq-8G9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "compare_all_results()"
      ],
      "metadata": {
        "id": "xBa5Gz0K9sTN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9999450a-1e18-4b21-a496-65dde4898499"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "            default_model |  Rademacher  |     SVHN     |  CIFAR-100   |   Average   \n",
            "=====================================================================================\n",
            "                Max Logit |    29.13%    |    9.16%     |    12.58%    |    16.96%   \n",
            "              Max Softmax |   *80.56%    |   *91.62%    |   *87.98%    |   *86.72%   \n",
            "            Cross Entropy |    29.09%    |    8.67%     |    12.61%    |    16.79%   \n",
            "                      ViM |    30.95%    |    21.30%    |    23.93%    |    25.39%   \n",
            "\n",
            "     pixmix_trained_model |  Rademacher  |     SVHN     |  CIFAR-100   |   Average   \n",
            "=====================================================================================\n",
            "                Max Logit |    5.59%     |    7.36%     |    12.52%    |    8.49%    \n",
            "  Max Softmax Probability |   *92.82%    |   *91.36%    |   *87.57%    |   *90.58%   \n",
            "            Cross Entropy |    5.77%     |    7.88%     |    13.62%    |    9.09%    \n",
            "                      ViM |    5.31%     |    8.22%     |    18.00%    |    10.51%   \n",
            "\n",
            "\n",
            "* highlights the maximum AUROC Score for an OOD Dataset\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "a5fa2dSlcnBm"
      },
      "outputs": [],
      "source": [
        "# Your output above should look similar to below"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "premium"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}